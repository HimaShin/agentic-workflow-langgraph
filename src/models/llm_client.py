def ask_llm(prompt):
    # Always return a fake response (for Streamlit Cloud only)
    return f"Reflected on result: Simulated response for: {prompt}"
